Train Dataset length: 5994
Test Dataset length: 5794
Dataset  Classes: 200
Loaded pretrained weights for efficientnet-b2
Model Summary:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         ZeroPad2d-1          [-1, 3, 225, 225]               0
Conv2dStaticSamePadding-2         [-1, 32, 112, 112]             864
       BatchNorm2d-3         [-1, 32, 112, 112]              64
MemoryEfficientSwish-4         [-1, 32, 112, 112]               0
         ZeroPad2d-5         [-1, 32, 114, 114]               0
Conv2dStaticSamePadding-6         [-1, 32, 112, 112]             288
       BatchNorm2d-7         [-1, 32, 112, 112]              64
MemoryEfficientSwish-8         [-1, 32, 112, 112]               0
          Identity-9             [-1, 32, 1, 1]               0
Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264
MemoryEfficientSwish-11              [-1, 8, 1, 1]               0
         Identity-12              [-1, 8, 1, 1]               0
Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288
         Identity-14         [-1, 32, 112, 112]               0
Conv2dStaticSamePadding-15         [-1, 16, 112, 112]             512
      BatchNorm2d-16         [-1, 16, 112, 112]              32
      MBConvBlock-17         [-1, 16, 112, 112]               0
        ZeroPad2d-18         [-1, 16, 114, 114]               0
Conv2dStaticSamePadding-19         [-1, 16, 112, 112]             144
      BatchNorm2d-20         [-1, 16, 112, 112]              32
MemoryEfficientSwish-21         [-1, 16, 112, 112]               0
         Identity-22             [-1, 16, 1, 1]               0
Conv2dStaticSamePadding-23              [-1, 4, 1, 1]              68
MemoryEfficientSwish-24              [-1, 4, 1, 1]               0
         Identity-25              [-1, 4, 1, 1]               0
Conv2dStaticSamePadding-26             [-1, 16, 1, 1]              80
         Identity-27         [-1, 16, 112, 112]               0
Conv2dStaticSamePadding-28         [-1, 16, 112, 112]             256
      BatchNorm2d-29         [-1, 16, 112, 112]              32
      MBConvBlock-30         [-1, 16, 112, 112]               0
         Identity-31         [-1, 16, 112, 112]               0
Conv2dStaticSamePadding-32         [-1, 96, 112, 112]           1,536
      BatchNorm2d-33         [-1, 96, 112, 112]             192
MemoryEfficientSwish-34         [-1, 96, 112, 112]               0
        ZeroPad2d-35         [-1, 96, 113, 113]               0
Conv2dStaticSamePadding-36           [-1, 96, 56, 56]             864
      BatchNorm2d-37           [-1, 96, 56, 56]             192
MemoryEfficientSwish-38           [-1, 96, 56, 56]               0
         Identity-39             [-1, 96, 1, 1]               0
Conv2dStaticSamePadding-40              [-1, 4, 1, 1]             388
MemoryEfficientSwish-41              [-1, 4, 1, 1]               0
         Identity-42              [-1, 4, 1, 1]               0
Conv2dStaticSamePadding-43             [-1, 96, 1, 1]             480
         Identity-44           [-1, 96, 56, 56]               0
Conv2dStaticSamePadding-45           [-1, 24, 56, 56]           2,304
      BatchNorm2d-46           [-1, 24, 56, 56]              48
      MBConvBlock-47           [-1, 24, 56, 56]               0
         Identity-48           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-49          [-1, 144, 56, 56]           3,456
      BatchNorm2d-50          [-1, 144, 56, 56]             288
MemoryEfficientSwish-51          [-1, 144, 56, 56]               0
        ZeroPad2d-52          [-1, 144, 58, 58]               0
Conv2dStaticSamePadding-53          [-1, 144, 56, 56]           1,296
      BatchNorm2d-54          [-1, 144, 56, 56]             288
MemoryEfficientSwish-55          [-1, 144, 56, 56]               0
         Identity-56            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-57              [-1, 6, 1, 1]             870
MemoryEfficientSwish-58              [-1, 6, 1, 1]               0
         Identity-59              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-60            [-1, 144, 1, 1]           1,008
         Identity-61          [-1, 144, 56, 56]               0
Conv2dStaticSamePadding-62           [-1, 24, 56, 56]           3,456
      BatchNorm2d-63           [-1, 24, 56, 56]              48
      MBConvBlock-64           [-1, 24, 56, 56]               0
         Identity-65           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-66          [-1, 144, 56, 56]           3,456
      BatchNorm2d-67          [-1, 144, 56, 56]             288
MemoryEfficientSwish-68          [-1, 144, 56, 56]               0
        ZeroPad2d-69          [-1, 144, 58, 58]               0
Conv2dStaticSamePadding-70          [-1, 144, 56, 56]           1,296
      BatchNorm2d-71          [-1, 144, 56, 56]             288
MemoryEfficientSwish-72          [-1, 144, 56, 56]               0
         Identity-73            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-74              [-1, 6, 1, 1]             870
MemoryEfficientSwish-75              [-1, 6, 1, 1]               0
         Identity-76              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-77            [-1, 144, 1, 1]           1,008
         Identity-78          [-1, 144, 56, 56]               0
Conv2dStaticSamePadding-79           [-1, 24, 56, 56]           3,456
      BatchNorm2d-80           [-1, 24, 56, 56]              48
      MBConvBlock-81           [-1, 24, 56, 56]               0
         Identity-82           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-83          [-1, 144, 56, 56]           3,456
      BatchNorm2d-84          [-1, 144, 56, 56]             288
MemoryEfficientSwish-85          [-1, 144, 56, 56]               0
        ZeroPad2d-86          [-1, 144, 60, 60]               0
Conv2dStaticSamePadding-87          [-1, 144, 28, 28]           3,600
      BatchNorm2d-88          [-1, 144, 28, 28]             288
MemoryEfficientSwish-89          [-1, 144, 28, 28]               0
         Identity-90            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-91              [-1, 6, 1, 1]             870
MemoryEfficientSwish-92              [-1, 6, 1, 1]               0
         Identity-93              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-94            [-1, 144, 1, 1]           1,008
         Identity-95          [-1, 144, 28, 28]               0
Conv2dStaticSamePadding-96           [-1, 48, 28, 28]           6,912
      BatchNorm2d-97           [-1, 48, 28, 28]              96
      MBConvBlock-98           [-1, 48, 28, 28]               0
         Identity-99           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-100          [-1, 288, 28, 28]          13,824
     BatchNorm2d-101          [-1, 288, 28, 28]             576
MemoryEfficientSwish-102          [-1, 288, 28, 28]               0
       ZeroPad2d-103          [-1, 288, 32, 32]               0
Conv2dStaticSamePadding-104          [-1, 288, 28, 28]           7,200
     BatchNorm2d-105          [-1, 288, 28, 28]             576
MemoryEfficientSwish-106          [-1, 288, 28, 28]               0
        Identity-107            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-108             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-109             [-1, 12, 1, 1]               0
        Identity-110             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-111            [-1, 288, 1, 1]           3,744
        Identity-112          [-1, 288, 28, 28]               0
Conv2dStaticSamePadding-113           [-1, 48, 28, 28]          13,824
     BatchNorm2d-114           [-1, 48, 28, 28]              96
     MBConvBlock-115           [-1, 48, 28, 28]               0
        Identity-116           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-117          [-1, 288, 28, 28]          13,824
     BatchNorm2d-118          [-1, 288, 28, 28]             576
MemoryEfficientSwish-119          [-1, 288, 28, 28]               0
       ZeroPad2d-120          [-1, 288, 32, 32]               0
Conv2dStaticSamePadding-121          [-1, 288, 28, 28]           7,200
     BatchNorm2d-122          [-1, 288, 28, 28]             576
MemoryEfficientSwish-123          [-1, 288, 28, 28]               0
        Identity-124            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-125             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-126             [-1, 12, 1, 1]               0
        Identity-127             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-128            [-1, 288, 1, 1]           3,744
        Identity-129          [-1, 288, 28, 28]               0
Conv2dStaticSamePadding-130           [-1, 48, 28, 28]          13,824
     BatchNorm2d-131           [-1, 48, 28, 28]              96
     MBConvBlock-132           [-1, 48, 28, 28]               0
        Identity-133           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-134          [-1, 288, 28, 28]          13,824
     BatchNorm2d-135          [-1, 288, 28, 28]             576
MemoryEfficientSwish-136          [-1, 288, 28, 28]               0
       ZeroPad2d-137          [-1, 288, 30, 30]               0
Conv2dStaticSamePadding-138          [-1, 288, 14, 14]           2,592
     BatchNorm2d-139          [-1, 288, 14, 14]             576
MemoryEfficientSwish-140          [-1, 288, 14, 14]               0
        Identity-141            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-142             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-143             [-1, 12, 1, 1]               0
        Identity-144             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-145            [-1, 288, 1, 1]           3,744
        Identity-146          [-1, 288, 14, 14]               0
Conv2dStaticSamePadding-147           [-1, 88, 14, 14]          25,344
     BatchNorm2d-148           [-1, 88, 14, 14]             176
     MBConvBlock-149           [-1, 88, 14, 14]               0
        Identity-150           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-151          [-1, 528, 14, 14]          46,464
     BatchNorm2d-152          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-153          [-1, 528, 14, 14]               0
       ZeroPad2d-154          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-155          [-1, 528, 14, 14]           4,752
     BatchNorm2d-156          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-157          [-1, 528, 14, 14]               0
        Identity-158            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-159             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-160             [-1, 22, 1, 1]               0
        Identity-161             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-162            [-1, 528, 1, 1]          12,144
        Identity-163          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-164           [-1, 88, 14, 14]          46,464
     BatchNorm2d-165           [-1, 88, 14, 14]             176
     MBConvBlock-166           [-1, 88, 14, 14]               0
        Identity-167           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-168          [-1, 528, 14, 14]          46,464
     BatchNorm2d-169          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-170          [-1, 528, 14, 14]               0
       ZeroPad2d-171          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-172          [-1, 528, 14, 14]           4,752
     BatchNorm2d-173          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-174          [-1, 528, 14, 14]               0
        Identity-175            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-176             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-177             [-1, 22, 1, 1]               0
        Identity-178             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-179            [-1, 528, 1, 1]          12,144
        Identity-180          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-181           [-1, 88, 14, 14]          46,464
     BatchNorm2d-182           [-1, 88, 14, 14]             176
     MBConvBlock-183           [-1, 88, 14, 14]               0
        Identity-184           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-185          [-1, 528, 14, 14]          46,464
     BatchNorm2d-186          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-187          [-1, 528, 14, 14]               0
       ZeroPad2d-188          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-189          [-1, 528, 14, 14]           4,752
     BatchNorm2d-190          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-191          [-1, 528, 14, 14]               0
        Identity-192            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-193             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-194             [-1, 22, 1, 1]               0
        Identity-195             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-196            [-1, 528, 1, 1]          12,144
        Identity-197          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-198           [-1, 88, 14, 14]          46,464
     BatchNorm2d-199           [-1, 88, 14, 14]             176
     MBConvBlock-200           [-1, 88, 14, 14]               0
        Identity-201           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-202          [-1, 528, 14, 14]          46,464
     BatchNorm2d-203          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-204          [-1, 528, 14, 14]               0
       ZeroPad2d-205          [-1, 528, 18, 18]               0
Conv2dStaticSamePadding-206          [-1, 528, 14, 14]          13,200
     BatchNorm2d-207          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-208          [-1, 528, 14, 14]               0
        Identity-209            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-210             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-211             [-1, 22, 1, 1]               0
        Identity-212             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-213            [-1, 528, 1, 1]          12,144
        Identity-214          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-215          [-1, 120, 14, 14]          63,360
     BatchNorm2d-216          [-1, 120, 14, 14]             240
     MBConvBlock-217          [-1, 120, 14, 14]               0
        Identity-218          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-219          [-1, 720, 14, 14]          86,400
     BatchNorm2d-220          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-221          [-1, 720, 14, 14]               0
       ZeroPad2d-222          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-223          [-1, 720, 14, 14]          18,000
     BatchNorm2d-224          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-225          [-1, 720, 14, 14]               0
        Identity-226            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-227             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-228             [-1, 30, 1, 1]               0
        Identity-229             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-230            [-1, 720, 1, 1]          22,320
        Identity-231          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-232          [-1, 120, 14, 14]          86,400
     BatchNorm2d-233          [-1, 120, 14, 14]             240
     MBConvBlock-234          [-1, 120, 14, 14]               0
        Identity-235          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-236          [-1, 720, 14, 14]          86,400
     BatchNorm2d-237          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-238          [-1, 720, 14, 14]               0
       ZeroPad2d-239          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-240          [-1, 720, 14, 14]          18,000
     BatchNorm2d-241          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-242          [-1, 720, 14, 14]               0
        Identity-243            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-244             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-245             [-1, 30, 1, 1]               0
        Identity-246             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-247            [-1, 720, 1, 1]          22,320
        Identity-248          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-249          [-1, 120, 14, 14]          86,400
     BatchNorm2d-250          [-1, 120, 14, 14]             240
     MBConvBlock-251          [-1, 120, 14, 14]               0
        Identity-252          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-253          [-1, 720, 14, 14]          86,400
     BatchNorm2d-254          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-255          [-1, 720, 14, 14]               0
       ZeroPad2d-256          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-257          [-1, 720, 14, 14]          18,000
     BatchNorm2d-258          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-259          [-1, 720, 14, 14]               0
        Identity-260            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-261             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-262             [-1, 30, 1, 1]               0
        Identity-263             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-264            [-1, 720, 1, 1]          22,320
        Identity-265          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-266          [-1, 120, 14, 14]          86,400
     BatchNorm2d-267          [-1, 120, 14, 14]             240
     MBConvBlock-268          [-1, 120, 14, 14]               0
        Identity-269          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-270          [-1, 720, 14, 14]          86,400
     BatchNorm2d-271          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-272          [-1, 720, 14, 14]               0
       ZeroPad2d-273          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-274            [-1, 720, 7, 7]          18,000
     BatchNorm2d-275            [-1, 720, 7, 7]           1,440
MemoryEfficientSwish-276            [-1, 720, 7, 7]               0
        Identity-277            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-278             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-279             [-1, 30, 1, 1]               0
        Identity-280             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-281            [-1, 720, 1, 1]          22,320
        Identity-282            [-1, 720, 7, 7]               0
Conv2dStaticSamePadding-283            [-1, 208, 7, 7]         149,760
     BatchNorm2d-284            [-1, 208, 7, 7]             416
     MBConvBlock-285            [-1, 208, 7, 7]               0
        Identity-286            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-287           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-288           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-289           [-1, 1248, 7, 7]               0
       ZeroPad2d-290         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-291           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-292           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-293           [-1, 1248, 7, 7]               0
        Identity-294           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-295             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-296             [-1, 52, 1, 1]               0
        Identity-297             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-298           [-1, 1248, 1, 1]          66,144
        Identity-299           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-300            [-1, 208, 7, 7]         259,584
     BatchNorm2d-301            [-1, 208, 7, 7]             416
     MBConvBlock-302            [-1, 208, 7, 7]               0
        Identity-303            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-304           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-305           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-306           [-1, 1248, 7, 7]               0
       ZeroPad2d-307         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-308           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-309           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-310           [-1, 1248, 7, 7]               0
        Identity-311           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-312             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-313             [-1, 52, 1, 1]               0
        Identity-314             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-315           [-1, 1248, 1, 1]          66,144
        Identity-316           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-317            [-1, 208, 7, 7]         259,584
     BatchNorm2d-318            [-1, 208, 7, 7]             416
     MBConvBlock-319            [-1, 208, 7, 7]               0
        Identity-320            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-321           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-322           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-323           [-1, 1248, 7, 7]               0
       ZeroPad2d-324         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-325           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-326           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-327           [-1, 1248, 7, 7]               0
        Identity-328           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-329             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-330             [-1, 52, 1, 1]               0
        Identity-331             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-332           [-1, 1248, 1, 1]          66,144
        Identity-333           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-334            [-1, 208, 7, 7]         259,584
     BatchNorm2d-335            [-1, 208, 7, 7]             416
     MBConvBlock-336            [-1, 208, 7, 7]               0
        Identity-337            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-338           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-339           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-340           [-1, 1248, 7, 7]               0
       ZeroPad2d-341         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-342           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-343           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-344           [-1, 1248, 7, 7]               0
        Identity-345           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-346             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-347             [-1, 52, 1, 1]               0
        Identity-348             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-349           [-1, 1248, 1, 1]          66,144
        Identity-350           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-351            [-1, 208, 7, 7]         259,584
     BatchNorm2d-352            [-1, 208, 7, 7]             416
     MBConvBlock-353            [-1, 208, 7, 7]               0
        Identity-354            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-355           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-356           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-357           [-1, 1248, 7, 7]               0
       ZeroPad2d-358           [-1, 1248, 9, 9]               0
Conv2dStaticSamePadding-359           [-1, 1248, 7, 7]          11,232
     BatchNorm2d-360           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-361           [-1, 1248, 7, 7]               0
        Identity-362           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-363             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-364             [-1, 52, 1, 1]               0
        Identity-365             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-366           [-1, 1248, 1, 1]          66,144
        Identity-367           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-368            [-1, 352, 7, 7]         439,296
     BatchNorm2d-369            [-1, 352, 7, 7]             704
     MBConvBlock-370            [-1, 352, 7, 7]               0
        Identity-371            [-1, 352, 7, 7]               0
Conv2dStaticSamePadding-372           [-1, 2112, 7, 7]         743,424
     BatchNorm2d-373           [-1, 2112, 7, 7]           4,224
MemoryEfficientSwish-374           [-1, 2112, 7, 7]               0
       ZeroPad2d-375           [-1, 2112, 9, 9]               0
Conv2dStaticSamePadding-376           [-1, 2112, 7, 7]          19,008
     BatchNorm2d-377           [-1, 2112, 7, 7]           4,224
MemoryEfficientSwish-378           [-1, 2112, 7, 7]               0
        Identity-379           [-1, 2112, 1, 1]               0
Conv2dStaticSamePadding-380             [-1, 88, 1, 1]         185,944
MemoryEfficientSwish-381             [-1, 88, 1, 1]               0
        Identity-382             [-1, 88, 1, 1]               0
Conv2dStaticSamePadding-383           [-1, 2112, 1, 1]         187,968
        Identity-384           [-1, 2112, 7, 7]               0
Conv2dStaticSamePadding-385            [-1, 352, 7, 7]         743,424
     BatchNorm2d-386            [-1, 352, 7, 7]             704
     MBConvBlock-387            [-1, 352, 7, 7]               0
        Identity-388            [-1, 352, 7, 7]               0
Conv2dStaticSamePadding-389           [-1, 1408, 7, 7]         495,616
     BatchNorm2d-390           [-1, 1408, 7, 7]           2,816
MemoryEfficientSwish-391           [-1, 1408, 7, 7]               0
AdaptiveAvgPool2d-392           [-1, 1408, 1, 1]               0
         Dropout-393                 [-1, 1408]               0
          Linear-394                  [-1, 512]         721,408
            ReLU-395                  [-1, 512]               0
         Dropout-396                  [-1, 512]               0
          Linear-397                  [-1, 200]         102,600
    EfficientNet-398                  [-1, 200]               0
================================================================
Total params: 8,525,002
Trainable params: 824,008
Non-trainable params: 7,700,994
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 310.01
Params size (MB): 32.52
Estimated Total Size (MB): 343.10
----------------------------------------------------------------
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 5.29346    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 0.00%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 5.28641    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 0.85%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 5.29410    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 0.60%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 5.29163    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 0.71%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 5.22258    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 0.91%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 5.25010    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 1.04%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 5.19344    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 1.38%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 5.19979    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 1.50%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 5.20311    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 1.81%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 5.12542    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 2.27%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 5.06776    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 2.38%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 5.08905    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 2.53%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 4.96685    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 2.94%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 5.02205    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 3.48%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 4.86790    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 3.68%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 4.83409    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 4.12%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 4.84593    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 4.41%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 4.73942    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 4.57%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 4.77199    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 4.66%
Epoch [1/10], Mean Train Loss: 4.3668, Mean Test Loss: 4.2473, Train Accuracy: 20.39%, Test Accuracy: 21.97%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 4.54427    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 6.25%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 4.45937    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 8.52%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 4.58159    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 11.01%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 4.37884    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 11.09%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 4.27696    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 11.97%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 4.08377    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 12.50%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 4.17871    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 12.70%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 4.21748    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 13.16%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.92425    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 13.35%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 4.00324    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 13.08%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 3.75326    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 13.37%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 3.93937    Batch Accuracy: 12.50%    Rolling Epoch Accuracy: 13.71%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 3.89242    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 13.79%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.75331    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 13.88%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.63806    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 14.41%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 3.75957    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 14.69%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 4.03451    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 14.95%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.78816    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 14.95%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 3.68662    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 15.19%
Epoch [2/10], Mean Train Loss: 3.3116, Mean Test Loss: 3.0823, Train Accuracy: 32.82%, Test Accuracy: 33.10%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 3.66342    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 15.62%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 3.81402    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 23.86%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 3.45812    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 23.51%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 3.27373    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 23.59%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 3.63346    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 23.86%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 3.52505    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 23.47%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 3.23215    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 23.72%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.08494    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 24.16%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.45751    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 23.77%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 3.62589    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 24.31%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 3.10918    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 24.16%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 3.09962    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 24.32%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 3.11795    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 24.38%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.53776    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 24.71%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.63995    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 24.80%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 3.22624    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 24.90%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.26311    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 24.69%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.37358    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 24.85%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 3.16679    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 25.16%
Epoch [3/10], Mean Train Loss: 2.8455, Mean Test Loss: 2.5917, Train Accuracy: 40.54%, Test Accuracy: 41.37%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 3.26613    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 25.00%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 3.11724    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 27.27%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 3.04478    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 29.02%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.99903    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 29.64%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.99459    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 29.65%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.87141    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 29.72%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.77906    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 29.15%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.30993    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 29.05%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.38848    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 29.48%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 3.11120    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 29.70%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.89727    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 29.86%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.95036    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 29.36%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 3.12495    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 29.42%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.10661    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 29.37%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.12245    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 29.83%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.98763    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 29.82%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.04977    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 29.79%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.04663    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 29.82%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.74302    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 29.68%
Epoch [4/10], Mean Train Loss: 2.5856, Mean Test Loss: 2.3200, Train Accuracy: 45.33%, Test Accuracy: 44.37%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.97255    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 21.88%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 3.03337    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 31.53%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.46043    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 32.44%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.87916    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 33.06%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.64213    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 32.85%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.58424    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 33.52%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.69556    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 33.15%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.17781    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 32.83%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.00265    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 33.18%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.46215    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 33.59%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 3.47737    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 33.35%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 3.26480    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 33.16%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.50300    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 33.21%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.55462    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 32.85%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.82934    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 32.67%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.48133    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 32.62%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.07247    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 32.51%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.44726    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 32.71%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.65253    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 32.73%
Epoch [5/10], Mean Train Loss: 2.3898, Mean Test Loss: 2.1479, Train Accuracy: 47.80%, Test Accuracy: 47.15%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.93916    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 34.38%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.70357    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 34.09%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.62579    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 35.71%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.90548    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 33.06%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.69005    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 32.55%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.66947    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 32.72%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.53584    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 33.20%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.25498    Batch Accuracy: 12.50%    Rolling Epoch Accuracy: 33.01%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.05748    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 33.26%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.27078    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 33.76%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.62960    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 34.16%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.92702    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 34.23%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.48282    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 34.30%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.95496    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 34.35%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.79967    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 34.51%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.57630    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 34.46%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.69396    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 34.59%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.78937    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 34.81%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.70817    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 35.05%
Epoch [6/10], Mean Train Loss: 2.2357, Mean Test Loss: 2.0156, Train Accuracy: 49.70%, Test Accuracy: 50.28%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.23159    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 53.12%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.27590    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 36.08%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.90320    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 36.01%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.70599    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.00%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.86714    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 36.74%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.60211    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 36.03%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.47599    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 36.07%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.66126    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 37.10%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.42067    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.04%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.46429    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 37.19%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.25964    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 37.25%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.29214    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 37.36%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.76342    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.71%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.48969    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.91%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.31849    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 37.99%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.61601    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.96%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.22827    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 38.00%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.60246    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 38.01%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.47802    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 38.09%
Epoch [7/10], Mean Train Loss: 2.1352, Mean Test Loss: 1.9253, Train Accuracy: 53.04%, Test Accuracy: 51.48%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.16445    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 40.62%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.70209    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.76%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.21257    Batch Accuracy: 56.25%    Rolling Epoch Accuracy: 41.52%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.03853    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 40.62%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.46808    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 40.85%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.23568    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 40.99%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.55349    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 40.42%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.22615    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 40.58%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.85247    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 40.28%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 3.11936    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 40.04%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.91793    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 39.29%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.75651    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 39.36%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.45571    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 39.33%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 1.85969    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 39.77%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.42988    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 39.52%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.46764    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 39.61%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.91577    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 39.65%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.87866    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 39.71%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.62688    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 39.71%
Epoch [8/10], Mean Train Loss: 2.0628, Mean Test Loss: 1.8746, Train Accuracy: 53.32%, Test Accuracy: 52.14%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.36045    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.50%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.52399    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 38.92%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.63983    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 39.58%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.61417    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.63%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.62201    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 41.31%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.58022    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 40.75%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.81012    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 40.98%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.59865    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 40.76%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.56342    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 40.59%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.66868    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 40.28%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.55110    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 40.16%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.68226    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 40.06%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.10694    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 39.93%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.00815    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 40.29%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.05474    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 40.12%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 3.06810    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 39.80%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.84086    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 39.77%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.10125    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 40.04%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.39701    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 40.19%
Epoch [9/10], Mean Train Loss: 1.9554, Mean Test Loss: 1.8271, Train Accuracy: 56.84%, Test Accuracy: 53.14%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.41688    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 31.25%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.12391    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 46.31%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.51043    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 46.28%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.78476    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 43.04%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.29690    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 42.99%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.34993    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 42.59%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.43530    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 43.49%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.86416    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 42.39%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.29468    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 42.17%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.47809    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.69%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.76212    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.55%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.62620    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.55%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.40962    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 41.48%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.63828    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.20%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.24933    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 41.11%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.17505    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 41.25%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.79973    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 41.07%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.42999    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 41.28%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.65156    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.21%
Epoch [10/10], Mean Train Loss: 1.9402, Mean Test Loss: 1.7768, Train Accuracy: 57.09%, Test Accuracy: 54.80%
Time elapsed: 777.845869
