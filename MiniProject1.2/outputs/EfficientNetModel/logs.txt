Train Dataset length: 5994
Test Dataset length: 5794
Dataset  Classes: 200
Loaded pretrained weights for efficientnet-b2
Model Summary:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         ZeroPad2d-1          [-1, 3, 225, 225]               0
Conv2dStaticSamePadding-2         [-1, 32, 112, 112]             864
       BatchNorm2d-3         [-1, 32, 112, 112]              64
MemoryEfficientSwish-4         [-1, 32, 112, 112]               0
         ZeroPad2d-5         [-1, 32, 114, 114]               0
Conv2dStaticSamePadding-6         [-1, 32, 112, 112]             288
       BatchNorm2d-7         [-1, 32, 112, 112]              64
MemoryEfficientSwish-8         [-1, 32, 112, 112]               0
          Identity-9             [-1, 32, 1, 1]               0
Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264
MemoryEfficientSwish-11              [-1, 8, 1, 1]               0
         Identity-12              [-1, 8, 1, 1]               0
Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288
         Identity-14         [-1, 32, 112, 112]               0
Conv2dStaticSamePadding-15         [-1, 16, 112, 112]             512
      BatchNorm2d-16         [-1, 16, 112, 112]              32
      MBConvBlock-17         [-1, 16, 112, 112]               0
        ZeroPad2d-18         [-1, 16, 114, 114]               0
Conv2dStaticSamePadding-19         [-1, 16, 112, 112]             144
      BatchNorm2d-20         [-1, 16, 112, 112]              32
MemoryEfficientSwish-21         [-1, 16, 112, 112]               0
         Identity-22             [-1, 16, 1, 1]               0
Conv2dStaticSamePadding-23              [-1, 4, 1, 1]              68
MemoryEfficientSwish-24              [-1, 4, 1, 1]               0
         Identity-25              [-1, 4, 1, 1]               0
Conv2dStaticSamePadding-26             [-1, 16, 1, 1]              80
         Identity-27         [-1, 16, 112, 112]               0
Conv2dStaticSamePadding-28         [-1, 16, 112, 112]             256
      BatchNorm2d-29         [-1, 16, 112, 112]              32
      MBConvBlock-30         [-1, 16, 112, 112]               0
         Identity-31         [-1, 16, 112, 112]               0
Conv2dStaticSamePadding-32         [-1, 96, 112, 112]           1,536
      BatchNorm2d-33         [-1, 96, 112, 112]             192
MemoryEfficientSwish-34         [-1, 96, 112, 112]               0
        ZeroPad2d-35         [-1, 96, 113, 113]               0
Conv2dStaticSamePadding-36           [-1, 96, 56, 56]             864
      BatchNorm2d-37           [-1, 96, 56, 56]             192
MemoryEfficientSwish-38           [-1, 96, 56, 56]               0
         Identity-39             [-1, 96, 1, 1]               0
Conv2dStaticSamePadding-40              [-1, 4, 1, 1]             388
MemoryEfficientSwish-41              [-1, 4, 1, 1]               0
         Identity-42              [-1, 4, 1, 1]               0
Conv2dStaticSamePadding-43             [-1, 96, 1, 1]             480
         Identity-44           [-1, 96, 56, 56]               0
Conv2dStaticSamePadding-45           [-1, 24, 56, 56]           2,304
      BatchNorm2d-46           [-1, 24, 56, 56]              48
      MBConvBlock-47           [-1, 24, 56, 56]               0
         Identity-48           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-49          [-1, 144, 56, 56]           3,456
      BatchNorm2d-50          [-1, 144, 56, 56]             288
MemoryEfficientSwish-51          [-1, 144, 56, 56]               0
        ZeroPad2d-52          [-1, 144, 58, 58]               0
Conv2dStaticSamePadding-53          [-1, 144, 56, 56]           1,296
      BatchNorm2d-54          [-1, 144, 56, 56]             288
MemoryEfficientSwish-55          [-1, 144, 56, 56]               0
         Identity-56            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-57              [-1, 6, 1, 1]             870
MemoryEfficientSwish-58              [-1, 6, 1, 1]               0
         Identity-59              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-60            [-1, 144, 1, 1]           1,008
         Identity-61          [-1, 144, 56, 56]               0
Conv2dStaticSamePadding-62           [-1, 24, 56, 56]           3,456
      BatchNorm2d-63           [-1, 24, 56, 56]              48
      MBConvBlock-64           [-1, 24, 56, 56]               0
         Identity-65           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-66          [-1, 144, 56, 56]           3,456
      BatchNorm2d-67          [-1, 144, 56, 56]             288
MemoryEfficientSwish-68          [-1, 144, 56, 56]               0
        ZeroPad2d-69          [-1, 144, 58, 58]               0
Conv2dStaticSamePadding-70          [-1, 144, 56, 56]           1,296
      BatchNorm2d-71          [-1, 144, 56, 56]             288
MemoryEfficientSwish-72          [-1, 144, 56, 56]               0
         Identity-73            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-74              [-1, 6, 1, 1]             870
MemoryEfficientSwish-75              [-1, 6, 1, 1]               0
         Identity-76              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-77            [-1, 144, 1, 1]           1,008
         Identity-78          [-1, 144, 56, 56]               0
Conv2dStaticSamePadding-79           [-1, 24, 56, 56]           3,456
      BatchNorm2d-80           [-1, 24, 56, 56]              48
      MBConvBlock-81           [-1, 24, 56, 56]               0
         Identity-82           [-1, 24, 56, 56]               0
Conv2dStaticSamePadding-83          [-1, 144, 56, 56]           3,456
      BatchNorm2d-84          [-1, 144, 56, 56]             288
MemoryEfficientSwish-85          [-1, 144, 56, 56]               0
        ZeroPad2d-86          [-1, 144, 60, 60]               0
Conv2dStaticSamePadding-87          [-1, 144, 28, 28]           3,600
      BatchNorm2d-88          [-1, 144, 28, 28]             288
MemoryEfficientSwish-89          [-1, 144, 28, 28]               0
         Identity-90            [-1, 144, 1, 1]               0
Conv2dStaticSamePadding-91              [-1, 6, 1, 1]             870
MemoryEfficientSwish-92              [-1, 6, 1, 1]               0
         Identity-93              [-1, 6, 1, 1]               0
Conv2dStaticSamePadding-94            [-1, 144, 1, 1]           1,008
         Identity-95          [-1, 144, 28, 28]               0
Conv2dStaticSamePadding-96           [-1, 48, 28, 28]           6,912
      BatchNorm2d-97           [-1, 48, 28, 28]              96
      MBConvBlock-98           [-1, 48, 28, 28]               0
         Identity-99           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-100          [-1, 288, 28, 28]          13,824
     BatchNorm2d-101          [-1, 288, 28, 28]             576
MemoryEfficientSwish-102          [-1, 288, 28, 28]               0
       ZeroPad2d-103          [-1, 288, 32, 32]               0
Conv2dStaticSamePadding-104          [-1, 288, 28, 28]           7,200
     BatchNorm2d-105          [-1, 288, 28, 28]             576
MemoryEfficientSwish-106          [-1, 288, 28, 28]               0
        Identity-107            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-108             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-109             [-1, 12, 1, 1]               0
        Identity-110             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-111            [-1, 288, 1, 1]           3,744
        Identity-112          [-1, 288, 28, 28]               0
Conv2dStaticSamePadding-113           [-1, 48, 28, 28]          13,824
     BatchNorm2d-114           [-1, 48, 28, 28]              96
     MBConvBlock-115           [-1, 48, 28, 28]               0
        Identity-116           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-117          [-1, 288, 28, 28]          13,824
     BatchNorm2d-118          [-1, 288, 28, 28]             576
MemoryEfficientSwish-119          [-1, 288, 28, 28]               0
       ZeroPad2d-120          [-1, 288, 32, 32]               0
Conv2dStaticSamePadding-121          [-1, 288, 28, 28]           7,200
     BatchNorm2d-122          [-1, 288, 28, 28]             576
MemoryEfficientSwish-123          [-1, 288, 28, 28]               0
        Identity-124            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-125             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-126             [-1, 12, 1, 1]               0
        Identity-127             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-128            [-1, 288, 1, 1]           3,744
        Identity-129          [-1, 288, 28, 28]               0
Conv2dStaticSamePadding-130           [-1, 48, 28, 28]          13,824
     BatchNorm2d-131           [-1, 48, 28, 28]              96
     MBConvBlock-132           [-1, 48, 28, 28]               0
        Identity-133           [-1, 48, 28, 28]               0
Conv2dStaticSamePadding-134          [-1, 288, 28, 28]          13,824
     BatchNorm2d-135          [-1, 288, 28, 28]             576
MemoryEfficientSwish-136          [-1, 288, 28, 28]               0
       ZeroPad2d-137          [-1, 288, 30, 30]               0
Conv2dStaticSamePadding-138          [-1, 288, 14, 14]           2,592
     BatchNorm2d-139          [-1, 288, 14, 14]             576
MemoryEfficientSwish-140          [-1, 288, 14, 14]               0
        Identity-141            [-1, 288, 1, 1]               0
Conv2dStaticSamePadding-142             [-1, 12, 1, 1]           3,468
MemoryEfficientSwish-143             [-1, 12, 1, 1]               0
        Identity-144             [-1, 12, 1, 1]               0
Conv2dStaticSamePadding-145            [-1, 288, 1, 1]           3,744
        Identity-146          [-1, 288, 14, 14]               0
Conv2dStaticSamePadding-147           [-1, 88, 14, 14]          25,344
     BatchNorm2d-148           [-1, 88, 14, 14]             176
     MBConvBlock-149           [-1, 88, 14, 14]               0
        Identity-150           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-151          [-1, 528, 14, 14]          46,464
     BatchNorm2d-152          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-153          [-1, 528, 14, 14]               0
       ZeroPad2d-154          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-155          [-1, 528, 14, 14]           4,752
     BatchNorm2d-156          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-157          [-1, 528, 14, 14]               0
        Identity-158            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-159             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-160             [-1, 22, 1, 1]               0
        Identity-161             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-162            [-1, 528, 1, 1]          12,144
        Identity-163          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-164           [-1, 88, 14, 14]          46,464
     BatchNorm2d-165           [-1, 88, 14, 14]             176
     MBConvBlock-166           [-1, 88, 14, 14]               0
        Identity-167           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-168          [-1, 528, 14, 14]          46,464
     BatchNorm2d-169          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-170          [-1, 528, 14, 14]               0
       ZeroPad2d-171          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-172          [-1, 528, 14, 14]           4,752
     BatchNorm2d-173          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-174          [-1, 528, 14, 14]               0
        Identity-175            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-176             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-177             [-1, 22, 1, 1]               0
        Identity-178             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-179            [-1, 528, 1, 1]          12,144
        Identity-180          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-181           [-1, 88, 14, 14]          46,464
     BatchNorm2d-182           [-1, 88, 14, 14]             176
     MBConvBlock-183           [-1, 88, 14, 14]               0
        Identity-184           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-185          [-1, 528, 14, 14]          46,464
     BatchNorm2d-186          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-187          [-1, 528, 14, 14]               0
       ZeroPad2d-188          [-1, 528, 16, 16]               0
Conv2dStaticSamePadding-189          [-1, 528, 14, 14]           4,752
     BatchNorm2d-190          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-191          [-1, 528, 14, 14]               0
        Identity-192            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-193             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-194             [-1, 22, 1, 1]               0
        Identity-195             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-196            [-1, 528, 1, 1]          12,144
        Identity-197          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-198           [-1, 88, 14, 14]          46,464
     BatchNorm2d-199           [-1, 88, 14, 14]             176
     MBConvBlock-200           [-1, 88, 14, 14]               0
        Identity-201           [-1, 88, 14, 14]               0
Conv2dStaticSamePadding-202          [-1, 528, 14, 14]          46,464
     BatchNorm2d-203          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-204          [-1, 528, 14, 14]               0
       ZeroPad2d-205          [-1, 528, 18, 18]               0
Conv2dStaticSamePadding-206          [-1, 528, 14, 14]          13,200
     BatchNorm2d-207          [-1, 528, 14, 14]           1,056
MemoryEfficientSwish-208          [-1, 528, 14, 14]               0
        Identity-209            [-1, 528, 1, 1]               0
Conv2dStaticSamePadding-210             [-1, 22, 1, 1]          11,638
MemoryEfficientSwish-211             [-1, 22, 1, 1]               0
        Identity-212             [-1, 22, 1, 1]               0
Conv2dStaticSamePadding-213            [-1, 528, 1, 1]          12,144
        Identity-214          [-1, 528, 14, 14]               0
Conv2dStaticSamePadding-215          [-1, 120, 14, 14]          63,360
     BatchNorm2d-216          [-1, 120, 14, 14]             240
     MBConvBlock-217          [-1, 120, 14, 14]               0
        Identity-218          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-219          [-1, 720, 14, 14]          86,400
     BatchNorm2d-220          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-221          [-1, 720, 14, 14]               0
       ZeroPad2d-222          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-223          [-1, 720, 14, 14]          18,000
     BatchNorm2d-224          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-225          [-1, 720, 14, 14]               0
        Identity-226            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-227             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-228             [-1, 30, 1, 1]               0
        Identity-229             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-230            [-1, 720, 1, 1]          22,320
        Identity-231          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-232          [-1, 120, 14, 14]          86,400
     BatchNorm2d-233          [-1, 120, 14, 14]             240
     MBConvBlock-234          [-1, 120, 14, 14]               0
        Identity-235          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-236          [-1, 720, 14, 14]          86,400
     BatchNorm2d-237          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-238          [-1, 720, 14, 14]               0
       ZeroPad2d-239          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-240          [-1, 720, 14, 14]          18,000
     BatchNorm2d-241          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-242          [-1, 720, 14, 14]               0
        Identity-243            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-244             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-245             [-1, 30, 1, 1]               0
        Identity-246             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-247            [-1, 720, 1, 1]          22,320
        Identity-248          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-249          [-1, 120, 14, 14]          86,400
     BatchNorm2d-250          [-1, 120, 14, 14]             240
     MBConvBlock-251          [-1, 120, 14, 14]               0
        Identity-252          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-253          [-1, 720, 14, 14]          86,400
     BatchNorm2d-254          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-255          [-1, 720, 14, 14]               0
       ZeroPad2d-256          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-257          [-1, 720, 14, 14]          18,000
     BatchNorm2d-258          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-259          [-1, 720, 14, 14]               0
        Identity-260            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-261             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-262             [-1, 30, 1, 1]               0
        Identity-263             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-264            [-1, 720, 1, 1]          22,320
        Identity-265          [-1, 720, 14, 14]               0
Conv2dStaticSamePadding-266          [-1, 120, 14, 14]          86,400
     BatchNorm2d-267          [-1, 120, 14, 14]             240
     MBConvBlock-268          [-1, 120, 14, 14]               0
        Identity-269          [-1, 120, 14, 14]               0
Conv2dStaticSamePadding-270          [-1, 720, 14, 14]          86,400
     BatchNorm2d-271          [-1, 720, 14, 14]           1,440
MemoryEfficientSwish-272          [-1, 720, 14, 14]               0
       ZeroPad2d-273          [-1, 720, 18, 18]               0
Conv2dStaticSamePadding-274            [-1, 720, 7, 7]          18,000
     BatchNorm2d-275            [-1, 720, 7, 7]           1,440
MemoryEfficientSwish-276            [-1, 720, 7, 7]               0
        Identity-277            [-1, 720, 1, 1]               0
Conv2dStaticSamePadding-278             [-1, 30, 1, 1]          21,630
MemoryEfficientSwish-279             [-1, 30, 1, 1]               0
        Identity-280             [-1, 30, 1, 1]               0
Conv2dStaticSamePadding-281            [-1, 720, 1, 1]          22,320
        Identity-282            [-1, 720, 7, 7]               0
Conv2dStaticSamePadding-283            [-1, 208, 7, 7]         149,760
     BatchNorm2d-284            [-1, 208, 7, 7]             416
     MBConvBlock-285            [-1, 208, 7, 7]               0
        Identity-286            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-287           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-288           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-289           [-1, 1248, 7, 7]               0
       ZeroPad2d-290         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-291           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-292           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-293           [-1, 1248, 7, 7]               0
        Identity-294           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-295             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-296             [-1, 52, 1, 1]               0
        Identity-297             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-298           [-1, 1248, 1, 1]          66,144
        Identity-299           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-300            [-1, 208, 7, 7]         259,584
     BatchNorm2d-301            [-1, 208, 7, 7]             416
     MBConvBlock-302            [-1, 208, 7, 7]               0
        Identity-303            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-304           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-305           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-306           [-1, 1248, 7, 7]               0
       ZeroPad2d-307         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-308           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-309           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-310           [-1, 1248, 7, 7]               0
        Identity-311           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-312             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-313             [-1, 52, 1, 1]               0
        Identity-314             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-315           [-1, 1248, 1, 1]          66,144
        Identity-316           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-317            [-1, 208, 7, 7]         259,584
     BatchNorm2d-318            [-1, 208, 7, 7]             416
     MBConvBlock-319            [-1, 208, 7, 7]               0
        Identity-320            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-321           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-322           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-323           [-1, 1248, 7, 7]               0
       ZeroPad2d-324         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-325           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-326           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-327           [-1, 1248, 7, 7]               0
        Identity-328           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-329             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-330             [-1, 52, 1, 1]               0
        Identity-331             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-332           [-1, 1248, 1, 1]          66,144
        Identity-333           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-334            [-1, 208, 7, 7]         259,584
     BatchNorm2d-335            [-1, 208, 7, 7]             416
     MBConvBlock-336            [-1, 208, 7, 7]               0
        Identity-337            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-338           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-339           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-340           [-1, 1248, 7, 7]               0
       ZeroPad2d-341         [-1, 1248, 11, 11]               0
Conv2dStaticSamePadding-342           [-1, 1248, 7, 7]          31,200
     BatchNorm2d-343           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-344           [-1, 1248, 7, 7]               0
        Identity-345           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-346             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-347             [-1, 52, 1, 1]               0
        Identity-348             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-349           [-1, 1248, 1, 1]          66,144
        Identity-350           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-351            [-1, 208, 7, 7]         259,584
     BatchNorm2d-352            [-1, 208, 7, 7]             416
     MBConvBlock-353            [-1, 208, 7, 7]               0
        Identity-354            [-1, 208, 7, 7]               0
Conv2dStaticSamePadding-355           [-1, 1248, 7, 7]         259,584
     BatchNorm2d-356           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-357           [-1, 1248, 7, 7]               0
       ZeroPad2d-358           [-1, 1248, 9, 9]               0
Conv2dStaticSamePadding-359           [-1, 1248, 7, 7]          11,232
     BatchNorm2d-360           [-1, 1248, 7, 7]           2,496
MemoryEfficientSwish-361           [-1, 1248, 7, 7]               0
        Identity-362           [-1, 1248, 1, 1]               0
Conv2dStaticSamePadding-363             [-1, 52, 1, 1]          64,948
MemoryEfficientSwish-364             [-1, 52, 1, 1]               0
        Identity-365             [-1, 52, 1, 1]               0
Conv2dStaticSamePadding-366           [-1, 1248, 1, 1]          66,144
        Identity-367           [-1, 1248, 7, 7]               0
Conv2dStaticSamePadding-368            [-1, 352, 7, 7]         439,296
     BatchNorm2d-369            [-1, 352, 7, 7]             704
     MBConvBlock-370            [-1, 352, 7, 7]               0
        Identity-371            [-1, 352, 7, 7]               0
Conv2dStaticSamePadding-372           [-1, 2112, 7, 7]         743,424
     BatchNorm2d-373           [-1, 2112, 7, 7]           4,224
MemoryEfficientSwish-374           [-1, 2112, 7, 7]               0
       ZeroPad2d-375           [-1, 2112, 9, 9]               0
Conv2dStaticSamePadding-376           [-1, 2112, 7, 7]          19,008
     BatchNorm2d-377           [-1, 2112, 7, 7]           4,224
MemoryEfficientSwish-378           [-1, 2112, 7, 7]               0
        Identity-379           [-1, 2112, 1, 1]               0
Conv2dStaticSamePadding-380             [-1, 88, 1, 1]         185,944
MemoryEfficientSwish-381             [-1, 88, 1, 1]               0
        Identity-382             [-1, 88, 1, 1]               0
Conv2dStaticSamePadding-383           [-1, 2112, 1, 1]         187,968
        Identity-384           [-1, 2112, 7, 7]               0
Conv2dStaticSamePadding-385            [-1, 352, 7, 7]         743,424
     BatchNorm2d-386            [-1, 352, 7, 7]             704
     MBConvBlock-387            [-1, 352, 7, 7]               0
        Identity-388            [-1, 352, 7, 7]               0
Conv2dStaticSamePadding-389           [-1, 1408, 7, 7]         495,616
     BatchNorm2d-390           [-1, 1408, 7, 7]           2,816
MemoryEfficientSwish-391           [-1, 1408, 7, 7]               0
AdaptiveAvgPool2d-392           [-1, 1408, 1, 1]               0
         Dropout-393                 [-1, 1408]               0
          Linear-394                  [-1, 512]         721,408
            ReLU-395                  [-1, 512]               0
         Dropout-396                  [-1, 512]               0
          Linear-397                  [-1, 200]         102,600
    EfficientNet-398                  [-1, 200]               0
================================================================
Total params: 8,525,002
Trainable params: 824,008
Non-trainable params: 7,700,994
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 310.01
Params size (MB): 32.52
Estimated Total Size (MB): 343.10
----------------------------------------------------------------
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 5.29346    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 0.00%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 5.28391    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 0.57%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 5.31692    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 0.60%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 5.28830    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 1.11%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 5.19064    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 1.14%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 5.18556    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 1.59%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 5.07935    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 2.10%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 5.02523    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 2.24%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 5.02302    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 2.62%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 4.76455    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 3.30%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 4.82605    Batch Accuracy: 0.00%    Rolling Epoch Accuracy: 3.37%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 4.77762    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 3.58%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 4.36369    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 3.98%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 4.59170    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 4.56%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 4.38211    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 4.74%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 4.24616    Batch Accuracy: 12.50%    Rolling Epoch Accuracy: 5.19%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 4.19458    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 5.59%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.98349    Batch Accuracy: 6.25%    Rolling Epoch Accuracy: 5.92%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 4.16757    Batch Accuracy: 12.50%    Rolling Epoch Accuracy: 6.16%
Epoch [1/10], Mean Train Loss: 3.5075, Mean Test Loss: 3.2959, Train Accuracy: 23.62%, Test Accuracy: 24.72%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 3.84944    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 15.62%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 3.76567    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 16.19%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 4.03161    Batch Accuracy: 3.12%    Rolling Epoch Accuracy: 16.67%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 3.89100    Batch Accuracy: 9.38%    Rolling Epoch Accuracy: 16.53%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 3.57268    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 17.91%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 3.54627    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 18.32%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 3.47882    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 18.44%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.63196    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 18.57%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.40730    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 18.60%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 3.37679    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 18.65%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 3.09455    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 19.18%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 3.42868    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 19.59%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 3.43181    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 19.71%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.19018    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 19.87%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.10805    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 20.28%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 3.50785    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 20.47%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.56679    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 20.69%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.44185    Batch Accuracy: 12.50%    Rolling Epoch Accuracy: 20.65%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 3.19982    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 20.86%
Epoch [2/10], Mean Train Loss: 2.6723, Mean Test Loss: 2.4443, Train Accuracy: 39.79%, Test Accuracy: 40.28%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 3.28151    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.12%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 3.35516    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 29.83%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 3.04446    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.12%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.90772    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 27.82%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 3.31712    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 27.90%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 3.15050    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 27.27%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.98547    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 27.51%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.64307    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 28.39%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.10174    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.32%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 3.41471    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.78%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.74853    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 28.81%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.81248    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 28.91%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.66314    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 28.82%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.36814    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 29.08%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.34552    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 29.17%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.91436    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 29.30%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.01202    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.82%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 3.16398    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 28.78%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.94351    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 28.94%
Epoch [3/10], Mean Train Loss: 2.3639, Mean Test Loss: 2.1613, Train Accuracy: 45.43%, Test Accuracy: 44.89%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.62697    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 43.75%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.79709    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 36.65%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.62090    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 35.42%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.61004    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 34.48%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.68788    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 33.99%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.61052    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 34.01%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.52932    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 33.91%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.21917    Batch Accuracy: 15.62%    Rolling Epoch Accuracy: 34.07%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 3.40865    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 34.80%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.74435    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 34.86%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.72790    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 34.90%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.65417    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 34.54%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.84045    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 34.19%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.90308    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 33.90%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.81551    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 34.13%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.65870    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 34.11%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.99979    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 34.14%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.86660    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 34.14%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.44451    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 34.01%
Epoch [4/10], Mean Train Loss: 2.1806, Mean Test Loss: 1.9976, Train Accuracy: 49.95%, Test Accuracy: 47.83%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.73527    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 34.38%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.81167    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 35.80%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.25671    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 36.01%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.50015    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 37.00%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.48720    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 36.20%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.34584    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 37.07%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.31163    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 37.14%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.95747    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 36.58%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.83622    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 37.08%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.37013    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 37.64%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 3.19377    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 37.07%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 3.08261    Batch Accuracy: 21.88%    Rolling Epoch Accuracy: 36.94%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.38275    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 37.14%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.55604    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 36.86%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.97825    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 36.50%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.22471    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 36.65%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 3.00481    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 36.37%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.34985    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 36.46%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.46961    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 36.43%
Epoch [5/10], Mean Train Loss: 2.0359, Mean Test Loss: 1.9077, Train Accuracy: 51.94%, Test Accuracy: 49.29%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.83921    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 31.25%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.69114    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 39.77%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.42180    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 40.18%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.69972    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.50%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.60246    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 36.59%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.53790    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 36.46%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.30000    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 36.94%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 3.34071    Batch Accuracy: 18.75%    Rolling Epoch Accuracy: 36.53%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.84547    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 36.61%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.08587    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 37.02%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.61200    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 36.88%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.72609    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.05%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.21527    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.29%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.64244    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.26%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.48819    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 37.21%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.38887    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 36.92%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.50331    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 37.11%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.79173    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.15%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.84802    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 37.29%
Epoch [6/10], Mean Train Loss: 1.9114, Mean Test Loss: 1.8116, Train Accuracy: 54.99%, Test Accuracy: 51.88%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.00188    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 50.00%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.19393    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.78%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.77728    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 37.50%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.64550    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 39.01%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.83939    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 37.80%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.41451    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 37.93%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.40286    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.81%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.50650    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 38.56%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.33860    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 38.70%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.37434    Batch Accuracy: 56.25%    Rolling Epoch Accuracy: 38.91%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.14845    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 38.92%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.08801    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 39.10%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.80476    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 39.13%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.43033    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 39.38%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.19808    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 39.30%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.41334    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 39.30%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.11140    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 39.40%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.58369    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 39.25%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.47059    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 39.24%
Epoch [7/10], Mean Train Loss: 1.8495, Mean Test Loss: 1.7674, Train Accuracy: 55.86%, Test Accuracy: 51.98%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 1.98973    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 53.12%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.55462    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 45.74%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.27223    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 44.94%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 1.93587    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 43.85%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.33300    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 43.67%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.15023    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 43.08%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.42389    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 42.16%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.28311    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.95%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.91435    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 41.74%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.91472    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 41.79%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.97066    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 41.24%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.71067    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 41.13%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.19963    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 41.22%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 1.77703    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 41.65%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 3.51312    Batch Accuracy: 25.00%    Rolling Epoch Accuracy: 41.27%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.33948    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.25%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.76066    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.11%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.67835    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.21%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.54662    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 41.23%
Epoch [8/10], Mean Train Loss: 1.7892, Mean Test Loss: 1.7524, Train Accuracy: 56.62%, Test Accuracy: 52.38%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.25851    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 43.75%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.63392    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 38.92%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.64773    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 41.96%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.39238    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 43.45%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.42356    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 43.06%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.51984    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 42.89%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.75135    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 42.98%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.44277    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 42.69%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.64179    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 42.48%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.59302    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 42.07%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.53898    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 41.55%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.58520    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 41.22%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.10851    Batch Accuracy: 53.12%    Rolling Epoch Accuracy: 41.09%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 3.02762    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.46%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 1.88897    Batch Accuracy: 50.00%    Rolling Epoch Accuracy: 41.49%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.96575    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 41.37%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.76501    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 41.17%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.14931    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.43%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.01926    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 41.64%
Epoch [9/10], Mean Train Loss: 1.6934, Mean Test Loss: 1.7283, Train Accuracy: 59.89%, Test Accuracy: 53.21%
Training Batch [0/188]    Batch Size: 32    Batch Mean Loss: 2.14991    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 37.50%
Training Batch [10/188]    Batch Size: 32    Batch Mean Loss: 2.05041    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 45.45%
Training Batch [20/188]    Batch Size: 32    Batch Mean Loss: 2.47381    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 46.43%
Training Batch [30/188]    Batch Size: 32    Batch Mean Loss: 2.92913    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 43.45%
Training Batch [40/188]    Batch Size: 32    Batch Mean Loss: 2.48549    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 43.06%
Training Batch [50/188]    Batch Size: 32    Batch Mean Loss: 2.16586    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 42.46%
Training Batch [60/188]    Batch Size: 32    Batch Mean Loss: 2.09258    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 43.44%
Training Batch [70/188]    Batch Size: 32    Batch Mean Loss: 2.88277    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 42.61%
Training Batch [80/188]    Batch Size: 32    Batch Mean Loss: 2.30321    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 42.79%
Training Batch [90/188]    Batch Size: 32    Batch Mean Loss: 2.35292    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 42.93%
Training Batch [100/188]    Batch Size: 32    Batch Mean Loss: 2.62875    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 42.98%
Training Batch [110/188]    Batch Size: 32    Batch Mean Loss: 2.76388    Batch Accuracy: 40.62%    Rolling Epoch Accuracy: 42.85%
Training Batch [120/188]    Batch Size: 32    Batch Mean Loss: 2.32375    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 42.74%
Training Batch [130/188]    Batch Size: 32    Batch Mean Loss: 2.59912    Batch Accuracy: 34.38%    Rolling Epoch Accuracy: 42.25%
Training Batch [140/188]    Batch Size: 32    Batch Mean Loss: 2.18367    Batch Accuracy: 46.88%    Rolling Epoch Accuracy: 42.11%
Training Batch [150/188]    Batch Size: 32    Batch Mean Loss: 2.36943    Batch Accuracy: 28.12%    Rolling Epoch Accuracy: 41.91%
Training Batch [160/188]    Batch Size: 32    Batch Mean Loss: 2.50948    Batch Accuracy: 31.25%    Rolling Epoch Accuracy: 41.61%
Training Batch [170/188]    Batch Size: 32    Batch Mean Loss: 2.34719    Batch Accuracy: 43.75%    Rolling Epoch Accuracy: 41.89%
Training Batch [180/188]    Batch Size: 32    Batch Mean Loss: 2.52281    Batch Accuracy: 37.50%    Rolling Epoch Accuracy: 41.80%
Epoch [10/10], Mean Train Loss: 1.6919, Mean Test Loss: 1.7098, Train Accuracy: 59.64%, Test Accuracy: 53.94%
Time elapsed: 774.351016
